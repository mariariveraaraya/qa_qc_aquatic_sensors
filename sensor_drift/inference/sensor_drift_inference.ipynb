{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f43a309c-f693-4192-bcf1-1c0e91f71b73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- This notebook uses a runid from the 'Experiments' tab or 'Models' label data.\n",
    "- This notebook assumes a model (selected from Experiments or Registered Models) have been chosen.\n",
    "- Change relevant parameters in main_param.py\n",
    "- Important: every time you change variables in main_param.py you need to detach and reattach the cluster by clicking in 'single node' in the right upper corner of the browser window (next to 'Run all') and selecting Dettach-reattach. Then click run again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7657104e-a8d5-4912-86de-39b884af0548",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Installing required libraries/modules and running required notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "714638db-95fc-415a-be97-0a0229fcddfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install packages that are not already part of the Databricks environment\n",
    "\n",
    "%pip install pyhydroqc==0.0.4\n",
    "%pip install detecta==0.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa1b467-9ce7-42f7-99e2-0582e0c3c271",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./sensor_utilities_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7af0213-205e-4780-95db-a36950b58de7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from parameters_inference import site_params,senid, calib_params, logged_model_arima\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyhydroqc\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from pyhydroqc import anomaly_utilities\n",
    "from pyhydroqc import calibration\n",
    "from pyhydroqc import model_workflow\n",
    "from pyhydroqc import rules_detect\n",
    "from pyhydroqc import modeling_utilities\n",
    "from pyhydroqc.model_workflow import ModelType\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from detecta import detect_cusum\n",
    "import plotly.express as px\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d4baa6-5b3a-4222-8f0d-24ebd35844a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Gather and clean data from a specified environment and sensor ID. The function doesn't take any arguments and returns two pandas DataFrames: `cleaned_df` and `filtered_df`.\n",
    "\n",
    "# Get the environment name from the environment variable\n",
    "env = os.getenv('tfenvironmentnameshort')\n",
    "\n",
    "# Call the 'call' function to upload the etdl file\n",
    "etdl_path = call('userupload', 'etdl', env, '/sensor_drift/measurements')\n",
    "\n",
    "# Clean the data using the 'clean' function\n",
    "cleaned_df = clean(etdl_path, f\"{senid.senid}\", f\"{senid.start_date}\", f\"{senid.end_date}\")\n",
    "\n",
    "# Filter the data using the 'filter_dataset' function\n",
    "filtered_df = filter_dataset(etdl_path, f\"{senid.senid}\", f\"{senid.start_date}\", f\"{senid.end_date}\")\n",
    "\n",
    "# Set the 'datetime' column as the index of the cleaned DataFrame\n",
    "cleaned_df = cleaned_df.set_index('datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19892723-b70e-4322-ab36-590fcba5c1f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Loading water quality measurement data and preprocessing\n",
    "\n",
    "This section also runs the arima model, including calibration parameters, ranges and persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c15bff9-2ba8-45e6-847a-689dd4cc1754",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Call data from blob \n",
    "\n",
    "site = 'East Trinity'\n",
    "sensors = senid.prm\n",
    "name = f\"{senid.senid}_{senid.start_date}_{senid.end_date}\"\n",
    "sensor_array = get_data(sensors= sensors, df = df_2)\n",
    "for snsr in sensor_array:\n",
    "    print(snsr + str(sensor_array[snsr]))\n",
    "    \n",
    "n_wqp =sensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "738dbae1-2f1e-4674-a213-c965da808124",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print selected parameters and check that ranges, windows and parameters make sense\n",
    "\n",
    "for snsr in sensors:\n",
    "    print(snsr + str(site_params[site][snsr]))\n",
    "print('calib' + str(calib_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0aec32d-e5ec-49b2-a47b-95a0b6ba982e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply ranges and persistence preprocessing\n",
    "\n",
    "range_count = dict()\n",
    "persist_count = dict()\n",
    "rules_metrics = dict()\n",
    "for snsr in sensor_array:\n",
    "    sensor_array[snsr], range_count[snsr] = pyhydroqc.range_check(df=sensor_array[snsr],\n",
    "                                                                     maximum=site_params[site][snsr].max_range,\n",
    "                                                                     minimum=site_params[site][snsr].min_range)\n",
    "    sensor_array[snsr], persist_count[snsr] = pyhydroqc.persistence(df=sensor_array[snsr],\n",
    "                                                                       length=120, \n",
    "                                                                       output_grp=True)\n",
    "    sensor_array[snsr] = pyhydroqc.interpolate(df=sensor_array[snsr])\n",
    "print('Rules based detection complete.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ae7814e-7e2b-428b-8d19-31d4fc7a1e43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Model detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd5fdda-08e0-4400-814e-29d89f8e5e26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get arima detections\n",
    "\n",
    "arima = dict()\n",
    "for snsr in sensors:\n",
    "    arima[snsr] = pyhydroqc.arima_detect(df=sensor_array[snsr], sensor=snsr, params= site_params[site][snsr],\n",
    "                                              rules=False, plots=False, summary=False, compare=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "322f185d-2757-4d77-b25b-bbf0c6c01955",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate model detections\n",
    "\n",
    "results_all = dict()\n",
    "for snsr in sensors:\n",
    "    models = dict()\n",
    "    models['arima'] = arima[snsr].df\n",
    "    results_all[snsr] = pyhydroqc.aggregate_results(df=sensor_array[snsr], \n",
    "                                                            models=models, \n",
    "                                                            verbose=True)\n",
    "    \n",
    "results_all[snsr].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a80c567-9fbd-4d8b-acef-f0078d07d36f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This cell is used to preprocess the data for CUSUM (Cumulative Sum) analyses.\n",
    "\n",
    "# Get the first sensor's data from the sensor_array dictionary\n",
    "first_sensor = list(sensor_array.values())[0]\n",
    "\n",
    "# Reset the index of the 'observed' column and store it in a new DataFrame gg\n",
    "first_sensor_reset = first_sensor['observed'].reset_index()\n",
    "\n",
    "# Keep only the 'observed' column in the DataFrame\n",
    "first_sensor_reset = first_sensor_reset[['observed']]\n",
    "\n",
    "# Initialize a StandardScaler, which will standardize features by removing the mean and scaling to unit variance\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# Initialize a MinMaxScaler, which scales and translates each feature individually such that it is in the given range on the training set, i.e between zero and one.\n",
    "mmscaler = MinMaxScaler()\n",
    "\n",
    "# Apply the StandardScaler to the 'observed' column and convert the result to a numpy array\n",
    "df_scaled = std_scaler.fit_transform(first_sensor_reset.to_numpy())\n",
    "\n",
    "# Convert the scaled data back to a DataFrame and name the column 'observed'\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=['observed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38263b53-090a-4779-87dc-f230865758af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run cusum analyses\n",
    "\n",
    "if f\"{n_wqp}\" == 'ph':\n",
    "    ta, tai, taf, amp = detect_cusum(df_scaled, 1, 0.4, True, True)\n",
    "    \n",
    "elif f\"{n_wqp}\" == 'cond':\n",
    "    ta, tai, taf, amp = detect_cusum(df_scaled, 1, 0.1, True, True)\n",
    "\n",
    "elif f\"{n_wqp}\" == 'temp':\n",
    "    ta, tai, taf, amp = detect_cusum(df_scaled, 1, 0.001, True, True)\n",
    "    \n",
    "start_df = pd.DataFrame(tai, columns= ['start'])\n",
    "end_df = pd.DataFrame(taf,  columns= ['finish'])\n",
    "\n",
    "end = start_df.join(end_df)\n",
    "sensor_df = sensor_array[snsr].copy()\n",
    "test_df = pd.DataFrame(results_all[snsr]).reset_index()\n",
    "\n",
    "start = test_df[['datetime']].rename(columns = {'datetime':'start'}).reset_index()\n",
    "\n",
    "final = test_df[['datetime']].rename(columns = {'datetime':'final'}).reset_index()\n",
    "\n",
    "\n",
    "# Write results back to lake\n",
    "\n",
    "intermediate = pd.concat([start, final], axis=1)\n",
    "\n",
    "intermediate_df = intermediate[['start', 'final']]\n",
    "spark.createDataFrame(intermediate_df).write.mode(\"overwrite\").parquet(f'abfss://lake-userupload@etdllake{env}.dfs.core.windows.net/sensor_drift/training/{name}_timestamp')\n",
    "\n",
    "results_df = sensor_array[snsr].reset_index()\n",
    "results_df = results_df.rename(columns={\"raw\": f\"{n_wqp}\"})\n",
    "spark.createDataFrame(results_df).write.mode(\"overwrite\").parquet(f'abfss://lake-userupload@etdllake{env}.dfs.core.windows.net/sensor_drift/training/{name}_timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "951ff6c1-8495-4b97-97b8-b86ebcd0829d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### R section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a8d753-1923-4c93-bc6c-ac59a82b5070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "library(sparklyr)\n",
    "library(glue)\n",
    "library(dplyr)\n",
    "library(lubridate)\n",
    "install.packages(c(\"mltest\", \"mlflow\", \"reticulate\"))\n",
    "library(mlflow)\n",
    "library(mltest)\n",
    "library(caret)\n",
    "library(tidyr)\n",
    "library(reticulate)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d0338bc-b293-4b8c-a6e0-858aec451c06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Read files produced with Python (above), variables defined by user, and files containing calibration, tides and rain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80520b92-1e96-4264-a2c3-e9b542503393",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Call files produced with Python. These were defined in main_param.py\n",
    "\n",
    "env = Sys.getenv('tfenvironmentnameshort')\n",
    "sc <- spark_connect(method = \"databricks\")\n",
    "file_path <-  glue('abfss://lake-userupload@etdllake{env}.dfs.core.windows.net/sensor_drift/rain/') \n",
    "file_path3 <-  glue('abfss://lake-userupload@etdllake{env}.dfs.core.windows.net/sensor_drift/cal_tps/')\n",
    "\n",
    "\n",
    "\n",
    "rainr_spark <- spark_read_parquet(sc, \n",
    "                            path = file_path,\n",
    "                            header = TRUE,\n",
    "                            infer_schema = TRUE)\n",
    "\n",
    "cal_spark <- spark_read_parquet(sc, \n",
    "                            path = file_path3,\n",
    "                            header = TRUE,\n",
    "                            infer_schema = TRUE)\n",
    "rainr<-collect(rainr_spark)\n",
    "\n",
    "calr<-collect(cal_spark)\n",
    "\n",
    "\n",
    "file_path5 <- glue('abfss://lake-userupload@etdllake{env}.dfs.core.windows.net/sensor_drift/tides_r/')\n",
    "\n",
    "\n",
    "tides_r_spark <- spark_read_parquet(sc, \n",
    "                            path = file_path5,\n",
    "                            header = TRUE,\n",
    "                            infer_schema = TRUE)\n",
    "\n",
    "tides_r<- collect(tides_r_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f69ea9-7f02-4dde-abe2-3801077b78f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Import a Python module. # Call parameters defined by user (location, sensor, model). These were defined in parameters_inference.py\n",
    "\n",
    "py2 <- import(\"main_param\")  \n",
    "senid <- py2$senid\n",
    "\n",
    "parameters<- glue(\"{senid}_{py2$start_date}_{py2$end_date}\")\n",
    "meas<- py2$prm\n",
    "meas<-meas[1]\n",
    "m2<-py2$logged_model_arima\n",
    "\n",
    "path_inference <-  glue('abfss://lake-userupload@etdllake{env}.dfs.core.windows.net/sensor_drift/inference')\n",
    "\n",
    "name<-parameters\n",
    "\n",
    "sensor_b<-senid\n",
    "\n",
    "r<-meas\n",
    "\n",
    "model<-m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b726b3af-354d-4484-bf82-3a1b5ccafafb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "#detections from model\n",
    "model_detections_csv<- spark_read_csv(sc, name = \"test_table\",  path = glue('{path_inference}/{name}_test.csv'), header=TRUE)\n",
    "model_detections <- collect(model_detections_csv)\n",
    "\n",
    "#detections from cusum\n",
    "cusum_detections <- spark_read_csv(sc, name = \"test_table\",  path = glue('{path_inference}/{name}_timestamp.csv'), header=TRUE)\n",
    "cusum_intervals <- collect(cusum_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9b1553c-2c78-4b1c-8b72-5909b5e942d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Loop over each date and interval to look for false positives\n",
    "\n",
    "false_positives- split(cusum_intervals, factor(cusum_intervals$start))\n",
    "fin <- vector(\"list\", length(dataset_1))\n",
    "for (i in seq_along(dataset_1)) {\n",
    "  \n",
    "  fin[[i]]<- interval((false_positives[[i]]$start), (false_positives[[i]]$final))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "715f2725-eae8-4dc4-b55f-ce9ecd9e0684",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Merging detections from model with cusum analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a153908-b37b-4af6-8b84-e4009e9d2382",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "# Identify detections from the model that are within the cusum detections\n",
    "selected_drift <- pickle_data$datetime %within% fin\n",
    "\n",
    "# Convert to a dataframe\n",
    "selected_drift_df <- as.data.frame(selected_drift)\n",
    "\n",
    "# Filter for TRUE values\n",
    "true_values <- selected_drift_df %>% filter(selected_drift_df == TRUE)\n",
    "\n",
    "# Combine the original data with the selected drift data\n",
    "combined_data <- cbind(pickle_data, selected_drift_df)\n",
    "combined_data$d_event <- as.logical(combined_data$detected_event)\n",
    "\n",
    "# Read the CSV file into a Spark dataframe\n",
    "spark_df <- spark_read_csv(sc, name = \"test_table\",  path = glue('{path_inference}/{name}_qual.csv'), header=TRUE)\n",
    "collected_data <- collect(spark_df)\n",
    "\n",
    "# Select relevant columns and convert datetime to datetime format\n",
    "quality_data <- collected_data %>% select(datetime, contains(glue('{r}_qual'))) %>% mutate(datetime = as_datetime(datetime))\n",
    "\n",
    "# Join the combined data with the quality data\n",
    "merged_data <- left_join(combined_data, quality_data)\n",
    "\n",
    "# Create a new column to indicate if a drift was detected by either the model or cusum\n",
    "model_cusum <- merged_data %>% mutate(drift_model_cusum = ifelse(d_event == 'TRUE' | sel_drift == 'TRUE', \"TRUE\", \"FALSE\"))\n",
    "model_cusum <- model_cusum %>% mutate(date = as_date(datetime))\n",
    "\n",
    "# Label the data according to requirements: '82' for doubtful and '83' for anomaly\n",
    "model_cusum <- model_cusum %>% mutate(sensor_qual= ifelse(d_event == 'TRUE' & sel_drift == 'TRUE', '83',\n",
    "                                                      ifelse((d_event == 'FALSE' & sel_drift == 'TRUE') | (d_event == 'TRUE' & sel_drift == 'FALSE') , '82',  model_cusum[,4])))\n",
    "# Filter for TRUE values in the drift_model_cusum column\n",
    "filtered_data <- model_cusum %>% filter(drift_model_cusum == 'TRUE')\n",
    "\n",
    "# Group by sensor_qual and count the number of occurrences\n",
    "grouped_data <- model_cusum %>% group_by(sensor_qual) %>% summarise(n=n())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd522b72-76a9-408f-bf1c-35f3e2fd53be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "\n",
    "# This cell is used to filter the tide data to account for false positives.\n",
    "\n",
    "# Add a 'date' column to 'tides_r', which is the 'time' column converted to date format\n",
    "# Group by 'date' and filter to keep only the rows where 'tide2' is 'HH' or 'LL'\n",
    "# Then, ungroup the data\n",
    "tides_with_date_df <- tides_r %>%\n",
    "  mutate(date = as_date(time)) %>%\n",
    "  group_by(date) %>%\n",
    "  filter(tide2 == 'HH' | tide2 == 'LL') %>%\n",
    "  ungroup()\n",
    "\n",
    "# Filter 'tides_with_date_df' to keep only the rows where 'level' is less than -1.2\n",
    "low_tide_levels_df <- tides_with_date_df %>% filter(level < -1.2)\n",
    "\n",
    "#max 2.14\n",
    "#min -1.76\n",
    "#median 0.132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e12144-3749-4b55-ba2f-3e309b57d137",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Include rain to account for false positives\n",
    "\n",
    "# Convert MeasurementDTTM to datetime format and extract the date\n",
    "rain_data = rainr %>%\n",
    "  mutate(data_timestamp = ymd_hms(MeasurementDTTM)) %>%\n",
    "  mutate(date = floor_date(data_timestamp, unit = \"day\")) %>%\n",
    "  mutate(date = as_date(date)) %>%\n",
    "  select(-Location)\n",
    "\n",
    "# Filter out NA values and select relevant columns\n",
    "filtered_rain_data = rainr %>%\n",
    "  filter(ramount_24hrtot != 'NA') %>%\n",
    "  mutate(date = as_date(MeasurementDTTM)) %>%\n",
    "  select(date, ramount_24hrtot)\n",
    "\n",
    "# Join the original and filtered rain data\n",
    "joined_rain_data = full_join(filtered_rain_data, rain_data)\n",
    "\n",
    "# Convert datetime to date format in the model_cusum dataframe\n",
    "model_cusum = model_cusum %>% mutate(date = as_date(datetime))\n",
    "\n",
    "# Join the model_cusum and joined_rain_data dataframes\n",
    "rain_event_data = left_join(model_cusum, joined_rain_data, by = 'date')\n",
    "\n",
    "# Create a new column to indicate if a drift was detected by either the model or cusum and there was more than 10mm of rain\n",
    "rain_event_data = rain_event_data %>%\n",
    "  mutate(rain = ifelse((sensor_qual == '82' | sensor_qual == '83') & ramount_24hrtot > 10, \"FP\", rain_event_data$drift_model_cusum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8654ef5d-55d4-4aac-b019-81486876e753",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Accounting for calibration\n",
    "\n",
    "# Filter the calibration data for the current sensor\n",
    "sensor_calibration_data <- calr %>% filter(series == glue('{sensor_b}'))\n",
    "\n",
    "# Split the data by calibration start time\n",
    "split_calibration_data <- split(sensor_calibration_data, factor(sensor_calibration_data$calibration_starttime))\n",
    "\n",
    "# Initialize a list to store the calibration intervals\n",
    "calibration_intervals <- vector(\"list\", length(split_calibration_data))\n",
    "\n",
    "# Loop through the split data and create intervals for each calibration period\n",
    "for (i in seq_along(split_calibration_data)) {\n",
    "  calibration_intervals[[i]] <- interval((split_calibration_data[[i]]$calibration_starttime), (split_calibration_data[[i]]$calibration_endtime))\n",
    "}\n",
    "\n",
    "# Check if the datetime in the pickle data is within the calibration intervals\n",
    "calibration_status <- pickle_data$datetime %within% calibration_intervals\n",
    "\n",
    "# Convert to a dataframe\n",
    "calibration_status_df <- as.data.frame(calibration_status)\n",
    "\n",
    "# Filter for TRUE values\n",
    "calibration_true_values <- calibration_status_df %>% filter(calibration_status_df == TRUE)\n",
    "\n",
    "# Combine the model_cusum data with the calibration status data\n",
    "combined_calibration_data <- cbind(model_cusum, calibration_status_df)\n",
    "\n",
    "# Convert detected_event to logical\n",
    "combined_calibration_data$d_event <- as.logical(combined_calibration_data$detected_event)\n",
    "\n",
    "# Filter for TRUE values in the d_event column\n",
    "detected_event_true_values <- combined_calibration_data %>% filter(d_event == TRUE)\n",
    "\n",
    "# Create a new column to indicate if a drift was detected by either the model or cusum and there was a calibration\n",
    "calibration_drift_data <- combined_calibration_data %>%\n",
    "  mutate(drift_cal = ifelse((sensor_qual == '82' | sensor_qual == '83') & calibration_status_df == \"TRUE\", \"FP\", combined_calibration_data$d_event))\n",
    "\n",
    "# Add a date column\n",
    "calibration_drift_data <- calibration_drift_data %>%\n",
    "  mutate(date = as_date(datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cb712b6-fc03-466f-9d4e-786d50319f55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Accounting for tidal changes\n",
    "\n",
    "# Select date and level columns\n",
    "tidal_data <- tides_with_date_df %>% select(date, level)\n",
    "\n",
    "# Filter for extreme tide levels and select relevant columns\n",
    "filtered_tidal_data <- tides_with_date_df %>% filter(level < -1.3 | level > 1.5) %>% select(date, level, tide)\n",
    "\n",
    "# Select relevant columns from model_cusum\n",
    "model_data <- tides_with_date_df %>% select(date, labeled_anomaly, drift_model_cusum, sensor_qual)\n",
    "\n",
    "# Join model_data and filtered_tidal_data\n",
    "joined_tidal_data <- left_join(model_data, filtered_tidal_data)\n",
    "\n",
    "# Filter for non-NA tide values and true drift_model_cusum values\n",
    "filtered_joined_tidal_data <- joined_tidal_data %>% filter(tide != 'NA' & drift_model_cusum == 'TRUE')\n",
    "\n",
    "# Create a new column to indicate if a drift was detected by either the model or cusum and there was an extreme tide\n",
    "tidal_drift_data <- joined_tidal_data %>% mutate(tide_drift = ifelse((sensor_qual == '82' | sensor_qual == '83') & (level < -1.3 | level > 1.5), 'FALSE', drift_model_cusum))\n",
    "\n",
    "# Filter for true drift_model_cusum values and non-NA tide_drift values\n",
    "filtered_tidal_drift_data <- tidal_drift_data %>% filter(drift_model_cusum == 'TRUE' & tide_drift != 'NA')\n",
    "\n",
    "# Get unique dates\n",
    "unique_dates <- filtered_tidal_drift_data %>% group_by(date) %>% unique()\n",
    "\n",
    "# Check if the date in the model_cusum dataframe is within the unique_dates\n",
    "date_status <- calibration_drift_data$date %in% unique_dates$date\n",
    "\n",
    "# Convert to a dataframe\n",
    "date_status_df <- as.data.frame(date_status)\n",
    "\n",
    "# Combine the model_cusum data with the date_status data\n",
    "combined_date_data <- cbind(calibration_drift_data, date_status_df)\n",
    "\n",
    "# Create a new column to indicate if a drift was detected by either the model or cusum and there was an extreme tide\n",
    "tide_drift_data <- combined_date_data %>% mutate(tides = ifelse((sensor_qual == '82' | sensor_qual == '83') & date_status_df == 'TRUE', 'FP', drift_model_cusum))\n",
    "\n",
    "# Merging all together\n",
    "\n",
    "# Select relevant columns\n",
    "tide_data <- tide_drift_data %>% select(datetime, tides)\n",
    "rain_data <- rain %>% select(datetime, rain)\n",
    "calibration_data <- cali %>% select(datetime, drift_cal)\n",
    "\n",
    "# Merge tide_data and rain_data\n",
    "merged_data <- merge(tide_data, rain_data, by = 'datetime')\n",
    "\n",
    "# Merge merged_data and calibration_data\n",
    "merged_all_data <- merge(merged_data, calibration_data, by = 'datetime')\n",
    "\n",
    "# Filter for FP values in the tides, rain, or drift_cal columns\n",
    "filtered_merged_data <- merged_all_data %>% filter(tides == 'FP' | rain == 'FP' | drift_cal == 'FP') %>% group_by(datetime) %>% slice_head() %>% ungroup() %>% mutate(sensor_qual = 'FALSE') %>% select(datetime, sensor_qual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6f51411-a3de-468b-ac99-bbaa1341c415",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Wrangling and writing back to Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52d00a3a-6079-46f8-ada6-1eca8299b54a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Select relevant columns and join with 'todo' dataframe\n",
    "selected_calibration_data <- calibration_drift_data %>% select(2:5, 14,15, 16) %>% left_join(todo) %>% ungroup()\n",
    "\n",
    "# Replace NA values in sensor_qual column with values from labeled_anomaly column\n",
    "updated_calibration_data <- selected_calibration_data %>% mutate(sensor_qual = ifelse(is.na(sensor_qual), labeled_anomaly, sensor_qual))\n",
    "\n",
    "# Filter for TRUE values in the drift_model_cusum column\n",
    "filtered_calibration_data <- updated_calibration_data %>% filter(drift_model_cusum == 'TRUE')\n",
    "\n",
    "# Rename columns starting with \"cond\", \"ph\", and \"temp\" to \"EC\", \"pH\", and \"WaterTemp\" respectively\n",
    "renamed_calibration_data <- updated_calibration_data %>% \n",
    "                            rename_if(startsWith(names(.), \"cond\"), ~paste0(\"EC\")) %>% \n",
    "                            rename_if(startsWith(names(.), \"ph\"), ~paste0(\"pH\")) %>% \n",
    "                            rename_if(startsWith(names(.), \"temp\"), ~paste0(\"WaterTemp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bef173f-23d4-4f42-b453-e436292bbe01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Plotting the data\n",
    "plot_data <- ggplot(df3, aes(x=datetime, y=df3[,2], col=sensor_qual)) + geom_point(size=1)\n",
    "\n",
    "# Renaming the second column of df3\n",
    "sensor_name <- names(df3[2])\n",
    "\n",
    "# Renaming columns starting with \"sensor\" to \"{sensor_name}_quality\" and adding a new column \"Location_code\"\n",
    "quality_data <- df3 %>% \n",
    "                rename_if(startsWith(names(.), \"sensor\"), ~paste0(glue('{sensor_name}_quality'))) %>%\n",
    "                mutate(Location_code = sensor_b)\n",
    "\n",
    "# Gathering the \"{sensor_name}_quality\" column into two columns: \"measurement_quality\" and \"measurement_quality_value\"\n",
    "quality_gathered_data <- gather(quality_data, measurement_quality, measurement_quality_value, glue('{sensor_name}_quality'))\n",
    "\n",
    "# Gathering the \"{sensor_name}\" column into two columns: \"measurement\" and \"measurement_value\"\n",
    "measurement_gathered_data <- gather(quality_gathered_data, measurement, measurement_value, glue('{sensor_name}'))\n",
    "\n",
    "# Adding a new column \"model\" with the value of \"{model}\"\n",
    "final_data <- measurement_gathered_data %>% mutate(model= glue('{model}') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c913f4c0-9bd8-4d58-88a3-3c49a74eba5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Write to lake\n",
    "\n",
    "filename = 'test'\n",
    "\n",
    "if(nrow(df6) == 0){\n",
    "  print(\"This data frame is empty\")\n",
    "}else{\n",
    "  d3 <- copy_to(sc, df6, overwrite= TRUE)\n",
    "\n",
    "  spark_write_parquet(\n",
    "  d3,\n",
    "  glue('abfss://lake-raw@etdllake{env}.dfs.core.windows.net/sensor_drift/inference'),\n",
    "  header = TRUE,\n",
    "  delimiter = \",\",\n",
    "  quote = \"\\\"\",\n",
    "  escape = \"\\\\\",\n",
    "  charset = \"UTF-8\",\n",
    "  null_value = NULL,\n",
    "  options = list(),\n",
    "  partition_by = NULL, mode = \"append\"\n",
    ")\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sensor_drift_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
