{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf07e536-786c-42b3-b8fc-8d7c0d531fe7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_parquet_file(dir, id, env, path):\n",
    "    \"\"\" This function is used to read parquet files from the data lake.\n",
    "    Arguments:\n",
    "        dir {string} -- The directory of the file to be read. \n",
    "        id {string} -- The id of the file to be read.\n",
    "        env {string} -- The environment of the file to be read.\n",
    "        path {string} -- The path of the file to be read.\n",
    "    Returns:\n",
    "        df -- The dataframe of the file to be read. \n",
    "    \"\"\"\n",
    "    env = os.getenv('tfenvironmentnameshort')\n",
    "    data_df = spark.read.parquet(\n",
    "        'abfss://lake-{0}@{1}lake{2}.dfs.core.windows.net{3}'.format(dir,id,env, path))\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def clean_data(df, sensor, start_date, end_date):\n",
    "    \"\"\" This function is used to clean the data. \n",
    "        Arguments:\n",
    "            df {dataframe} -- The dataframe to be cleaned. \n",
    "            sensor {string} -- The sensor to be cleaned. \n",
    "            start_date {string} -- The start date of the data to be cleaned. \n",
    "            end_date {string} -- The end date of the data to be cleaned.\n",
    "\n",
    "        Returns: \n",
    "            cleaned_data_df {dataframe} -- The cleaned dataframe.\n",
    "        \"\"\"\n",
    "    \n",
    "    sensor_data_df = df.filter(df.Location_code == sensor)\n",
    "    sensor_data_df = sensor_data_df.select('MeasurementDTTM', 'EC', 'pH', 'WaterTemp', 'EC_quality', 'pH_quality', 'WaterTemp_quality')\n",
    "    sensor_data_df = sensor_data_df.withColumnRenamed(\"MeasurementDTTM\",\"datetime\").withColumnRenamed(\"EC\",\"cond\").withColumnRenamed(\"pH\", \"ph\").withColumnRenamed(\"WaterTemp\", \"temp\").withColumnRenamed(\"EC_quality\", \"cond_qual\").withColumnRenamed(\"pH_quality\", \"ph_qual\").withColumnRenamed(\"WaterTemp_quality\", \"temp_qual\")\n",
    "    sensor_data_df=sensor_data_df.withColumn(\"cond\", sensor_data_df.cond/1000)\n",
    "    sensor_data_pandas_df = sensor_data_df.toPandas()\n",
    "\n",
    "    sensor_data_pandas_df = sensor_data_pandas_df.assign(cond_qual = [True if cond_qual == 93 or cond_qual == 92 else False for cond_qual in sensor_data_pandas_df['cond_qual']])\n",
    "    sensor_data_pandas_df = sensor_data_pandas_df.assign(ph_qual = [True if ph_qual == 93 or ph_qual == 92 else False for ph_qual in sensor_data_pandas_df['ph_qual']])\n",
    "    sensor_data_pandas_df = sensor_data_pandas_df.assign(temp_qual = [True if temp_qual == 93 or temp_qual == 92 else False for temp_qual in sensor_data_pandas_df['temp_qual']])\n",
    "\n",
    "    sensor_data_pandas_df = sensor_data_pandas_df.sort_values('datetime')\n",
    "    sorted_data_df= sensor_data_pandas_df.set_index('datetime')\n",
    "\n",
    "    cleaned_data_df = sorted_data_df.reset_index()\n",
    "    cleaned_data_df = cleaned_data_df[(cleaned_data_df.datetime> start_date)]\n",
    "    cleaned_data_df = cleaned_data_df[(cleaned_data_df.datetime< end_date)]\n",
    "    cleaned_data_df['cond'] = cleaned_data_df['cond'].astype(float)\n",
    "    cleaned_data_df['ph'] = cleaned_data_df['ph'].astype(float)\n",
    "    cleaned_data_df['temp'] = cleaned_data_df['temp'].astype(float)\n",
    "\n",
    "    return cleaned_data_df\n",
    "\n",
    "\n",
    "def filter_data(df, sensor, start_date, end_date):\n",
    "    \"\"\" This function is used to filter the data. It does not create a csv file, as opposed to the function clean.\n",
    "    Arguments: \n",
    "        df {dataframe} -- The dataframe to be filtered.\n",
    "        sensor {string} -- The sensor to be filtered.\n",
    "        start_date {string} -- The start date of the data to be filtered.\n",
    "        end_date {string} -- The end date of the data to be filtered.\n",
    "\n",
    "    Returns:\n",
    "        filtered_data_df{dataframe} -- The filtered dataframe.\n",
    "    \"\"\"\n",
    "    sensor_data_df = df.filter(df.Location_code == sensor)\n",
    "    sensor_data_df = sensor_data_df.select('MeasurementDTTM', 'EC', 'pH', 'WaterTemp', 'EC_quality', 'pH_quality', 'WaterTemp_quality')\n",
    "    sensor_data_df = sensor_data_df.withColumnRenamed(\"MeasurementDTTM\",\"datetime\").withColumnRenamed(\"EC\",\"cond\").withColumnRenamed(\"pH\", \"ph\").withColumnRenamed(\"WaterTemp\", \"temp\").withColumnRenamed(\"EC_quality\", \"cond_qual\").withColumnRenamed(\"pH_quality\", \"ph_qual\").withColumnRenamed(\"WaterTemp_quality\", \"temp_qual\")\n",
    "    sensor_data_df=sensor_data_df.withColumn(\"cond\", sensor_data_df.cond/1000)\n",
    "    sensor_data_pandas_df = sensor_data_df.toPandas()\n",
    "    sensor_data_pandas_df = sensor_data_pandas_df.sort_values('datetime')\n",
    "    sorted_data_df= sensor_data_pandas_df.set_index('datetime')\n",
    "\n",
    "    filtered_data_df = sorted_data_df.reset_index()\n",
    "    filtered_data_df = filtered_data_df[(filtered_data_df.datetime> start_date)]\n",
    "    filtered_data_df = filtered_data_df[(filtered_data_df.datetime< end_date)]\n",
    "\n",
    "    spark.createDataFrame(filtered_data_df).write.mode('overwrite').option('header','true').csv(f'abfss://lake-userupload@etdllakedev.dfs.core.windows.net/sensor_drift/training/{sensor}_{start_date}_{end_date}_qual.csv')\n",
    "\n",
    "    return filtered_data_df\n",
    "\n",
    "def get_data(sensors, df):\n",
    "    \"\"\"\n",
    "    get_data imports time series data from csv files. Files may specified explicitly by file name, or a series of files\n",
    "    may be imported that follow a naming pattern with site and year (e.g. \"MainStreet2014.csv\").\n",
    "    Files should have columns corresponding to each sensor. If technician labels and corrections exist, they may be\n",
    "    imported by naming columns sensor_cor and labeled_anomaly.\n",
    "    Arguments:\n",
    "        sensors: list of name(s) of the sensor/variable data of interest. These must be the column names in data file(s).\n",
    "        filename: string of the file name containing input data\n",
    "        site: string of name of the data collection site\n",
    "        years: list of the year(s) of interest\n",
    "        path: path to .csv files containing the data of interest\n",
    "    Returns:\n",
    "        sensor_array: array of pandas DataFrames, each with 3 columns for the variable/sensor of interest:\n",
    "        'raw', 'cor', 'labeled_anomaly'.\n",
    "    \"\"\"\n",
    "    # create data frames with raw, corrected, and labeled data (if the corrected and labeled data exist)\n",
    "    df_full = df\n",
    "    sensor_array = dict()\n",
    "    for snsr in sensors:\n",
    "        df = []\n",
    "        df = pd.DataFrame(index=df_full.index)\n",
    "        df['raw'] = df_full[snsr]\n",
    "\n",
    "        # if corrected data is available in dataset\n",
    "        if snsr + '_cor' in df_full.columns:\n",
    "            df['cor'] = df_full[snsr + '_cor']\n",
    "        if snsr + \"_qual\" in df_full.columns:\n",
    "            df['labeled_anomaly'] = df_full[snsr + '_qual']\n",
    "        sensor_array[snsr] = df\n",
    "\n",
    "    return sensor_array"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sensor_utilities_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
