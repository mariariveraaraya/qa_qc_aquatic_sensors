{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25b3c36e-0be9-4c86-b6a2-46c2623f000c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- This notebook is used to experiment with new parameters and evaluate relevant metrics.\n",
    "- Start with the file _readme_ in the parent folder. This file provides context and troubleshooting information.\n",
    "- Continue with parameters_training.py. Once you have followed the instructions there, click 'Run all' in this notebook. Check out the Table of Contents on the left to navigate to different sections.\n",
    "- Once this notebook has been successfully run, query results using the sensor_drift_training_visualisation notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfedd987-70f3-4e62-be0c-7a831b639246",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Installing required libraries/modules and running required notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39150de3-1038-4bf8-a8ca-151664ec976d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install packages that are not already part of the Databricks environment\n",
    "# The first one is a modified wheel created from https://pypi.org/project/pyhydroqc/. The purpose of this modified wheel is to include parameters in the mlflow runs as part of the experimentation phase\n",
    "\n",
    "%pip install install /dbfs/FileStore/pyhydroqc-mod-0.0.4-py2.py3-none-any.whl\n",
    "%pip install detecta==0.0.5\n",
    "%pip install ipytest==0.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca953dd-0d2e-4f23-8d46-f082be387a31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./sensor_utilities_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08e46b03-8e50-487b-b5eb-fd357492fb7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from parameters_training import site_params,senid, calib_params\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyhydroqc\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from pyhydroqc import anomaly_utilities\n",
    "from pyhydroqc import calibration\n",
    "from pyhydroqc import model_workflow\n",
    "from pyhydroqc import rules_detect\n",
    "from pyhydroqc import modeling_utilities\n",
    "from pyhydroqc.model_workflow import ModelType\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from detecta import detect_cusum\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "import ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "832fcea6-638d-4709-b7a3-61c25ad23170",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Gather and clean data from a specified environment and sensor ID. The function doesn't take any arguments and returns two pandas DataFrames: `cleaned_df` and `filtered_df`.\n",
    "\n",
    "# Get the environment name from the environment variable\n",
    "env = os.getenv('platform_env')\n",
    "\n",
    "# Call the 'call' function to upload the etdl file\n",
    "etdl_path = read_parquet_file('userupload', 'etdl', env, '/sensor_drift/measurements')\n",
    "\n",
    "# Clean the data using the 'clean2' function\n",
    "cleaned_df = clean_data(etdl_path, f\"{senid.senid}\", f\"{senid.start_date}\", f\"{senid.end_date}\")\n",
    "\n",
    "# Filter the data using the 'filter_ds2' function\n",
    "filtered_df = filter_data(etdl_path, f\"{senid.senid}\", f\"{senid.start_date}\", f\"{senid.end_date}\")\n",
    "\n",
    "# Set the 'datetime' column as the index of the cleaned DataFrame\n",
    "cleaned_df = cleaned_df.set_index('datetime')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07af9194-2fdc-4cce-bfcf-c2b28017c28c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Making sure quality coded data including non acceptable and doubtiful is being used (92 and 93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119c1491-16bd-4d5d-b94d-1d18cae1c605",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed8df80e-c5af-4740-a21a-8ff0d6e11a94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if any of the observations in the cleaned dataframe have a quality code of True (92/93)\n",
    "try:\n",
    "    assert (cleaned_df[f'{senid.prm[0]}_qual'].eq(True)).any(), \"Invalid Operation\"\n",
    "except AssertionError as msg:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39458144-2658-470a-a180-2b34798e0321",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "# Define a test function to check if the cleaned_df has any True values in the specified column\n",
    "def test_example():\n",
    "    assert(cleaned_df[f'{senid.prm[0]}_qual'].eq(True)).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "106fb935-1562-4d2a-b7d5-350872242bba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Loading water quality measurement data and preprocessing\n",
    "\n",
    "This section also runs the arima model, including calibration parameters, ranges and persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a50b970b-a884-4270-ad04-ed83b58ac2f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Call data from blob\n",
    "\n",
    "# Define the site name\n",
    "site = 'East Trinity'\n",
    "\n",
    "# Get the list of sensors\n",
    "sensors = senid.prm\n",
    "\n",
    "# Define the name for the dataset\n",
    "name = f\"{senid.senid}_{senid.start_date}_{senid.end_date}\"\n",
    "\n",
    "# Get the data for the sensors from the cleaned dataframe\n",
    "sensor_array = get_data(sensors=sensors, df=cleaned_df)\n",
    "\n",
    "# Print the data for each sensor in the sensor_array\n",
    "for sensor in sensor_array:\n",
    "    print(sensor + str(sensor_array[sensor]))\n",
    "\n",
    "# Set the first sensor in the list as the n_wqp\n",
    "n_wqp = sensors[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04b82945-0a74-4cac-aaf6-b44b37e3caf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply ranges and persistence preprocessing\n",
    "\n",
    "range_count = dict()\n",
    "persist_count = dict()\n",
    "rules_metrics = dict()\n",
    "\n",
    "# Apply range check and update range count\n",
    "for snsr in sensor_array:\n",
    "    sensor_array[snsr], range_count[snsr] = pyhydroqc.range_check(df=sensor_array[snsr],\n",
    "                                                                 maximum=site_params[site][snsr].max_range,\n",
    "                                                                 minimum=site_params[site][snsr].min_range)\n",
    "\n",
    "    # Apply persistence check and update persistence count\n",
    "    sensor_array[snsr], persist_count[snsr] = pyhydroqc.persistence(df=sensor_array[snsr],\n",
    "                                                                   length=120, #site_params[site][snsr].persist,\n",
    "                                                                   output_grp=True)\n",
    "\n",
    "    # Interpolate missing values\n",
    "    sensor_array[snsr] = pyhydroqc.interpolate(df=sensor_array[snsr])\n",
    "\n",
    "print('Rules based detection complete.\\n')\n",
    "\n",
    "# Print site parameters and calibration parameters\n",
    "for snsr in sensors:\n",
    "    print(snsr + str(site_params[site][snsr]))\n",
    "\n",
    "print('calib' + str(calib_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f06f228-a182-41a4-8b51-642e62027a12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Model detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f64615e1-ee81-472a-9c06-ef3b40d6b667",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get arima detections\n",
    "arima = dict()\n",
    "for sensor in sensors:\n",
    "    arima[sensor] = pyhydroqc.arima_detect(df=sensor_array[sensor], sensor=sensor, params=site_params[site][sensor], rules=False, plots=False, summary=False, compare=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a230652-c295-4614-a6d0-b07fc4599f9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the results for all sensors\n",
    "\n",
    "results_all = dict()\n",
    "\n",
    "# Loop over each sensor in the provided list\n",
    "for sensor in sensors:\n",
    "    # Initialize a dictionary to store the models for the current sensor\n",
    "    models = dict()\n",
    "    # Store the ARIMA model for the current sensor in the models dictionary\n",
    "    models['arima'] = arima[sensor].df\n",
    "    \n",
    "    # Use the PyHydroQC library to aggregate the results for the current sensor\n",
    "    # The results are stored in the results_all dictionary\n",
    "    results_all[sensor] = pyhydroqc.aggregate_results(df=sensor_array[sensor], \n",
    "                                                     models=models, \n",
    "                                                     verbose=True, \n",
    "                                                     compare=True)\n",
    "# Get the sensor data for the last sensor in the list    \n",
    "df_test = sensor_array[sensor]\n",
    "# Rename the 'raw' column to the name of the sensor and reset the DataFrame's index\n",
    "df_test = df_test.rename(columns={\"raw\": f\"{n_wqp}\"}).reset_index()\n",
    "\n",
    "# Convert the DataFrame to a Spark DataFrame and save it as a Parquet file in the specified location\n",
    "spark.createDataFrame(df_test).write.mode(\"overwrite\").parquet(f'abfss://lake-userupload@etdllake{env}.dfs.core.windows.net/sensor_drift/training/{name}_test')\n",
    "\n",
    "# Get the sensor data for the last sensor in the list\n",
    "df_sensor_array = sensor_array[sensor]\n",
    "\n",
    "# Convert the results for the last sensor to a DataFrame and reset the DataFrame's index\n",
    "df_results = pd.DataFrame(results_all[sensor][0]).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8549286b-8922-4316-a655-3969f4b92f7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Visualise model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d2b7e1-5b5e-4150-9a98-d2e550e91f59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This cell is optional and is used to visualize labeled anomalies in the data.\n",
    "\n",
    "# Convert the results for the last sensor to a DataFrame and reset the DataFrame's index\n",
    "test = pd.DataFrame(results_all[sensor][0]).reset_index()\n",
    "\n",
    "# Create a scatter plot of the 'observed' values over time, with the points colored by whether they are labeled as anomalies\n",
    "fig = px.scatter(test, x=\"datetime\", y=\"observed\", color = \"labeled_anomaly\")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()\n",
    "\n",
    "# This cell is optional and is used to visualize detected anomalies in the data.\n",
    "\n",
    "# Create a scatter plot of the 'observed' values over time, with the points colored by whether they are detected as events\n",
    "fig = px.scatter(test, x=\"datetime\", y=\"observed\", color = \"detected_event\")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "671c77e5-6ece-4ee4-9852-b8425529f57e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cummulative sum analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0517d632-d306-4129-8e62-4b927c35f40e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This cell is used to preprocess the data for CUSUM (Cumulative Sum) analyses.\n",
    "\n",
    "# Get the first sensor's data from the sensor_array dictionary\n",
    "first_sensor = list(sensor_array.values())[0]\n",
    "\n",
    "# Reset the index of the 'observed' column and store it in a new DataFrame gg\n",
    "first_sensor_reset = first_sensor['observed'].reset_index()\n",
    "\n",
    "# Keep only the 'observed' column in the DataFrame\n",
    "first_sensor_reset = first_sensor_reset[['observed']]\n",
    "\n",
    "# Initialize a StandardScaler, which will standardize features by removing the mean and scaling to unit variance\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# Initialize a MinMaxScaler, which scales and translates each feature individually such that it is in the given range on the training set, i.e between zero and one.\n",
    "mmscaler = MinMaxScaler()\n",
    "\n",
    "# Apply the StandardScaler to the 'observed' column and convert the result to a numpy array\n",
    "df_scaled = std_scaler.fit_transform(first_sensor_reset.to_numpy())\n",
    "\n",
    "# Convert the scaled data back to a DataFrame and name the column 'observed'\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=['observed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92963e89-1402-40b7-b147-2b8b433f3446",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This cell is used to run CUSUM (Cumulative Sum) analyses on the preprocessed data.\n",
    "\n",
    "# Check the value of n_wqp and run the appropriate CUSUM analysis\n",
    "if f\"{n_wqp}\" == 'ph':\n",
    "    # If n_wqp is 'ph', run CUSUM with a threshold of 1 and drift of 0.4\n",
    "    ta, tai, taf, amp = detect_cusum(df_scaled, 1, 0.4, True, True)\n",
    "    \n",
    "elif f\"{n_wqp}\" == 'cond':\n",
    "    # If n_wqp is 'cond', run CUSUM with a threshold of 1 and drift of 0.009\n",
    "    ta, tai, taf, amp = detect_cusum(df_scaled, 1, 0.009, True, True)\n",
    "    \n",
    "elif f\"{n_wqp}\" == 'temp':\n",
    "    # If n_wqp is 'temp', run CUSUM with a threshold of 1 and drift of 0.001\n",
    "    ta, tai, taf, amp = detect_cusum(df_scaled, 1, 0.001, True, True)\n",
    "\n",
    "# Convert the start and finish indices of the detected changes to DataFrames\n",
    "dfs = pd.DataFrame(tai, columns= ['start'])\n",
    "dfe = pd.DataFrame(taf,  columns= ['finish'])\n",
    "\n",
    "# Join the start and finish DataFrames to create a DataFrame of the detected changes\n",
    "end = dfs.join(dfe)\n",
    "\n",
    "# Get the start and finish times of the detected changes from the test DataFrame\n",
    "s = test.loc[end.start]\n",
    "s = s[['datetime']].rename(columns = {'datetime':'start'}).reset_index()\n",
    "\n",
    "f= test.loc[end.finish]\n",
    "f = f[['datetime']].rename(columns = {'datetime':'final'}).reset_index()\n",
    "\n",
    "# Concatenate the start and finish times to create a DataFrame of the detected changes with timestamps\n",
    "inter = pd.concat([s, f], axis=1)\n",
    "\n",
    "# Keep only the 'start' and 'final' columns\n",
    "inter2 = inter[['start', 'final']]\n",
    "\n",
    "# Convert the DataFrame to a Spark DataFrame and save it as a Parquet file in the specified location\n",
    "spark.createDataFrame(inter2).write.mode(\"overwrite\").parquet(f'abfss://lake-userupload@etdllake{env}.dfs.core.windows.net/sensor_drift/training/{name}_timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a35a9e0a-140b-4566-a429-ff4867d80841",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### R section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bbfd85e-c59c-414c-a380-0e2155676fdb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce504867-392e-487f-94e2-ff40c1386412",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Import libraries\n",
    "\n",
    "library(sparklyr)\n",
    "library(glue)\n",
    "library(dplyr)\n",
    "library(lubridate)\n",
    "install.packages(c(\"mltest\", \"mlflow\", \"reticulate\"))\n",
    "library(mlflow)\n",
    "library(mltest)\n",
    "library(caret)\n",
    "library(tidyr)\n",
    "library(reticulate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4c36faf-e739-4b5c-8a5a-4d6ea8f77da6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Read files produced with Python (above), variables defined by user, and files containing calibration, tides and rain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42051a25-f928-43b0-adc3-a45fe02fbf29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "# This cell is used to load the required data from Parquet files.\n",
    "\n",
    "# Get the short name of the TensorFlow environment\n",
    "env = Sys.getenv('tfenvironmentnameshort')\n",
    "\n",
    "# Connect to Spark using the Databricks method\n",
    "sc <- spark_connect(method = \"databricks\")\n",
    "\n",
    "# Define the paths to the Parquet files\n",
    "file_path <-  glue('abfss://lake-raw@etdllake{env}.dfs.core.windows.net/sensor_drift/datasets/rain_historical/') \n",
    "file_path3 <-  glue('abfss://lake-raw@etdllake{env}.dfs.core.windows.net/sensor_drift/datasets/cal_tps/')\n",
    "file_path5 <- glue('abfss://lake-raw@etdllake{env}.dfs.core.windows.net/sensor_drift/datasets/tides_r/')\n",
    "\n",
    "# Load the Parquet files into Spark DataFrames\n",
    "rainr_spark <- spark_read_parquet(sc, \n",
    "                            path = file_path,\n",
    "                            header = TRUE,\n",
    "                            infer_schema = TRUE)\n",
    "\n",
    "cal_spark <- spark_read_parquet(sc, \n",
    "                            path = file_path3,\n",
    "                            header = TRUE,\n",
    "                            infer_schema = TRUE)\n",
    "\n",
    "tides_r_spark <- spark_read_parquet(sc, \n",
    "                            path = file_path5,\n",
    "                            header = TRUE,\n",
    "                            infer_schema = TRUE)\n",
    "\n",
    "# Collect the Spark DataFrames into R DataFrames\n",
    "rainr<-collect(rainr_spark)\n",
    "calr<-collect(cal_spark)\n",
    "tides_r<- collect(tides_r_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcc32a64-e64e-4800-9820-ca464a074312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "\n",
    "# Import Python module containing the parameters defined by the user (location, sensor, model). These were defined in parameters_inference.py\n",
    "py <- import(\"parameters_training\")  # Replace \"module_name\" with the name of the Python module\n",
    "\n",
    "# Get the sensor ID from the imported Python module\n",
    "x <- py$senid\n",
    "\n",
    "# Call a Python function from the imported module to get the sensor ID\n",
    "senid <- x$senid\n",
    "\n",
    "# Construct a string of parameters including the sensor ID, start date, and end date\n",
    "parameters <- glue(\"{x$senid}_{py$start_date}_{py$end_date}\")\n",
    "\n",
    "# Get the measurement parameter from the imported Python module\n",
    "meas<- x$prm\n",
    "\n",
    "# If there are multiple measurement parameters, keep only the first one\n",
    "meas<-meas[1]\n",
    "\n",
    "# Set the name to the constructed string of parameters\n",
    "name<-parameters\n",
    "\n",
    "# Set the sensor_b to the sensor ID\n",
    "sensor_b<-senid\n",
    "\n",
    "# Set r to the measurement parameter\n",
    "r<-meas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f8e309b-23a8-4a98-ad95-427ce89aa9e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# This cell is used to load the model detections and CUSUM detections from Parquet files.\n",
    "\n",
    "# Define the path to the training data\n",
    "training_path <-  glue('abfss://lake-userupload@etdllake{env}.dfs.core.windows.net/sensor_drift/training/')\n",
    "\n",
    "# Load the model detections from a Parquet file into a Spark DataFrame\n",
    "model_detections_spark <- spark_read_parquet(sc, name = \"model_detections_table\",  path = glue('{training_path}/{name}_test'), header=TRUE)\n",
    "\n",
    "# Collect the Spark DataFrame into an R DataFrame\n",
    "model_detections <- collect(model_detections_spark)\n",
    "\n",
    "# Load the CUSUM detections from a Parquet file into a Spark DataFrame\n",
    "cusum_detections_spark <- spark_read_parquet(sc, name = \"cusum_detections_table\",  path = glue('{training_path}/{name}_timestamp'), header=TRUE)\n",
    "\n",
    "# Collect the Spark DataFrame into an R DataFrame\n",
    "cusum_detections <- collect(cusum_detections_spark)\n",
    "\n",
    "# Add a 'date' column to the model_detections DataFrame, which is the 'datetime' column converted to date format\n",
    "model_detections_with_date <- model_detections %>%\n",
    "  mutate(date = as_date(datetime))\n",
    "\n",
    "# Loop over each date and interval to look for false positives\n",
    "intervals <- split(cusum_detections, factor(cusum_detections$start))\n",
    "final_intervals <- vector(\"list\", length(intervals))\n",
    "for (i in seq_along(intervals)) {\n",
    "  final_intervals[[i]] <- interval((intervals[[i]]$start), (intervals[[i]]$final))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bfbe060-6723-41c4-a1fe-d904488dbd94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Merging detections from model with cusum analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52d405ff-3e78-4ecb-97b4-3200a5e9b047",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# This cell is used to identify which model detections are within the CUSUM detections.\n",
    "\n",
    "# Check if the 'datetime' values in model_detections are within the 'fin' interval\n",
    "model_detections_within_interval <- model_detections$datetime %within% fin\n",
    "\n",
    "# Convert the result to a data frame\n",
    "model_detections_df <- as.data.frame(model_detections_within_interval)\n",
    "\n",
    "# Filter the data frame to keep only the rows where 'model_detections_df' is TRUE\n",
    "true_detections_df <- model_detections_df %>% filter(model_detections_df == TRUE)\n",
    "\n",
    "# Combine 'model_detections' and 'model_detections_df' into a single data frame\n",
    "combined_detections_df <- cbind(model_detections, model_detections_df)\n",
    "\n",
    "# Convert the 'detected_event' column to logical values\n",
    "combined_detections_df$d_event <- as.logical(combined_detections_df$detected_event)\n",
    "\n",
    "# Filter the data frame to keep only the rows where 'd_event' is TRUE\n",
    "true_events_df <- combined_detections_df %>% filter(d_event == TRUE)\n",
    "\n",
    "# Load the quality data from a CSV file into a Spark DataFrame\n",
    "quality_data_spark_df <- spark_read_csv(sc, name = \"test_table\",  path = glue('{path_training}/{name}_qual.csv'), header=TRUE)\n",
    "\n",
    "# Collect the Spark DataFrame into an R DataFrame\n",
    "quality_data_df  <- collect(quality_data_spark_df)\n",
    "\n",
    "# Select the 'datetime' and quality columns from 'quality_data_df', and convert 'datetime' to datetime format\n",
    "quality_data_selected_df <- quality_data_df %>% select(datetime, contains(glue('{r}_qual'))) %>% mutate(datetime = as_datetime(datetime))\n",
    "\n",
    "# Join 'combined_detections_df' and 'quality_data_selected_df' on the 'datetime' column\n",
    "combined_quality_detections_df <- left_join(combined_detections_df, quality_data_selected_df)\n",
    "\n",
    "# Add a 'drift_model_cusum' column to 'combined_quality_detections_df', which is TRUE if either 'd_event' or 'model_detections_df' is TRUE, and FALSE otherwise\n",
    "model_cusum_df <- combined_quality_detections_df %>% mutate(drift_model_cusum = ifelse(d_event == 'TRUE' | model_detections_df == 'TRUE', \"TRUE\", \"FALSE\"))\n",
    "\n",
    "# Add a 'date' column to 'model_cusum_df', which is the 'datetime' column converted to date format\n",
    "model_cusum_df <- model_cusum_df %>% mutate(date = as_date(datetime))\n",
    "\n",
    "# Label according to requirements of machine-label doubtful '82' and anomaly '83'\n",
    "\n",
    "model_cusum_df <- model_cusum_df %>% mutate(sensor_qual= ifelse(d_event == 'TRUE' & model_detections_df == 'TRUE', '83',\n",
    "                                                      ifelse(d_event == 'FALSE' & model_detections_df == 'TRUE', '82', model_cusum_df[, 15])))\n",
    "\n",
    "true_cusum_df <- model_cusum_df %>% filter(drift_model_cusum == 'TRUE')\n",
    "model_cusum_df %>% group_by(sensor_qual) %>% summarise(n=n())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a59e6298-396d-4aba-8ceb-6e108965b1e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####  Include tidal, rainfall and calibration data to account for false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d281c4b8-aa4c-4489-8305-505576f1e149",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# This cell is used to filter the tide data to account for false positives.\n",
    "\n",
    "# Add a 'date' column to 'tides_r', which is the 'time' column converted to date format\n",
    "# Group by 'date' and filter to keep only the rows where 'tide2' is 'HH' or 'LL'\n",
    "# Then, ungroup the data\n",
    "tides_with_date_df <- tides_r %>%\n",
    "  mutate(date = as_date(time)) %>%\n",
    "  group_by(date) %>%\n",
    "  filter(tide2 == 'HH' | tide2 == 'LL') %>%\n",
    "  ungroup()\n",
    "\n",
    "# Filter 'tides_with_date_df' to keep only the rows where 'level' is less than -1.2\n",
    "low_tide_levels_df <- tides_with_date_df %>% filter(level < -1.2)\n",
    "\n",
    "#max 2.14\n",
    "#min -1.76\n",
    "#median 0.132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4266d924-4976-4cd1-84bd-8c389e4ee6f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# This cell is used to preprocess the rain data and join it with the model detections.\n",
    "\n",
    "# Convert 'data_timestamp' to datetime format, add a 'date' column which is 'data_timestamp' rounded down to the nearest day\n",
    "# Group by 'date' and calculate the total rain amount for each day, ignoring NA values\n",
    "# Convert 'date' to date format\n",
    "rain_data_df = rainr %>%\n",
    "  mutate(data_timestamp = ymd_hms(data_timestamp)) %>%\n",
    "  mutate(date = floor_date(data_timestamp, unit = \"day\")) %>%\n",
    "  group_by(date) %>%\n",
    "  summarise(ramount_24hrtot = sum(ramount_10mintot, na.rm = TRUE)) %>%\n",
    "  ungroup() %>%\n",
    "  mutate(date = as_date(date))\n",
    "\n",
    "# Filter 'rainr' to keep only the rows where 'ramount_24hrtot' is not NA and 'station' is one of the specified values\n",
    "# Add a 'date' column which is 'data_timestamp' converted to date format, and keep only the 'date' and 'ramount_24hrtot' columns\n",
    "filtered_rain_data_df <- rainr %>%\n",
    "  filter(ramount_24hrtot != 'NA') %>%\n",
    "  filter(station == 'FirewoodBund1wqt3' | station == 'HillsBundwqt1' | station == 'etdl_path SPH') %>%\n",
    "  mutate(date = as_date(data_timestamp)) %>%\n",
    "  select(date, ramount_24hrtot)\n",
    "\n",
    "# Combine 'filtered_rain_data_df' and 'rain_data_df' into a single data frame\n",
    "combined_rain_data_df <- rbind(filtered_rain_data_df, rain_data_df)\n",
    "\n",
    "# Join 'filtered_rain_data_df' and 'rain_data_df' on common columns\n",
    "joined_rain_data_df <- full_join(filtered_rain_data_df, rain_data_df)\n",
    "\n",
    "# Convert 'datetime' to date format in 'model_cusum'\n",
    "model_cusum_df <- model_cusum %>% mutate(date = as_date(datetime))\n",
    "\n",
    "# Join 'model_cusum_df' and 'joined_rain_data_df' on the 'date' column\n",
    "rain_and_model_data_df = left_join(model_cusum_df, joined_rain_data_df, by = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd4f3b36-98f3-49eb-abd4-20b6ea569e17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# This cell is used to account for calibration in the sensor data.\n",
    "\n",
    "# Filter 'calr' to keep only the rows where 'series' is equal to 'sensor_b'\n",
    "sensor_data_df <- calr %>% filter(series == glue('{sensor_b}'))\n",
    "\n",
    "# Split 'sensor_data_df' into a list of data frames, each containing the data for one calibration start time\n",
    "calibration_datasets_list <- split(sensor_data_df, factor(sensor_data_df$calibration_starttime))\n",
    "\n",
    "# Initialize a list to store the calibration intervals\n",
    "calibration_intervals_list <- vector(\"list\", length(calibration_datasets_list))\n",
    "\n",
    "# For each data frame in 'calibration_datasets_list', create an interval from the calibration start time to the calibration end time\n",
    "for (i in seq_along(calibration_datasets_list)) {\n",
    "  calibration_intervals_list[[i]] <- interval((calibration_datasets_list[[i]]$calibration_starttime), (calibration_datasets_list[[i]]$calibration_endtime))\n",
    "}\n",
    "\n",
    "# Check if the 'datetime' values in 'model_detections' are within the calibration intervals\n",
    "model_detections_within_calibration <- model_detections$datetime %within% calibration_intervals_list\n",
    "\n",
    "# Convert the result to a data frame\n",
    "model_detections_within_calibration_df <- as.data.frame(model_detections_within_calibration)\n",
    "\n",
    "# Filter 'model_detections_within_calibration_df' to keep only the rows where 'model_detections_within_calibration_df' is TRUE\n",
    "true_detections_df <- model_detections_within_calibration_df %>% filter(model_detections_within_calibration_df == TRUE)\n",
    "\n",
    "# Combine 'model_cusum' and 'model_detections_within_calibration_df' into a single data frame\n",
    "combined_detections_df <- cbind(model_cusum, model_detections_within_calibration_df)\n",
    "\n",
    "# Convert the 'detected_event' column to logical values\n",
    "combined_detections_df$d_event <- as.logical(combined_detections_df$detected_event)\n",
    "\n",
    "# Filter 'combined_detections_df' to keep only the rows where 'd_event' is TRUE\n",
    "true_events_df <- combined_detections_df %>% filter(d_event == TRUE)\n",
    "\n",
    "# Print the structure of 'true_events_df'\n",
    "str(true_events_df)\n",
    "\n",
    "# Add a 'drift_cal' column to 'combined_detections_df', which is 'FP' if both 'd_event' and 'model_detections_within_calibration_df' are TRUE, and 'd_event' otherwise\n",
    "calibrated_detections_df <- combined_detections_df %>% mutate(drift_cal = ifelse(d_event == \"TRUE\" & model_detections_within_calibration_df == \"TRUE\", \"FP\", combined_detections_df$d_event))\n",
    "\n",
    "# Add a 'date' column to 'calibrated_detections_df', which is the 'datetime' column converted to date format\n",
    "calibrated_detections_with_date_df <- calibrated_detections_df %>% mutate(date = as_date(datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f27a48-3947-4288-a9d7-ab9fdf73577b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# The following code is used to add a 'drift_status' column to the 'calibrated_detections_with_date_df' dataframe.\n",
    "# This column indicates whether the sensor was drifting at the time of each measurement.\n",
    "# If 'drift_model_cusum' is 'TRUE' and 'ramount_24hrtot' is greater than 10, 'drift_status' is 'FP'.\n",
    "# Otherwise, 'drift_status' is the same as 'drift_model_cusum'.\n",
    "\n",
    "rain_data_with_drift <- calibrated_detections_with_date_df %>%\n",
    "mutate(drift_status = ifelse(drift_model_cusum == 'TRUE' & ramount_24hrtot > 10, \"FP\", calibrated_detections_with_date_df$drift_model_cusum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91f29a0c-9331-4ce7-9107-33dbc96a149f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Accounting for tidal changes\n",
    "\n",
    "# Filter the tide data to keep only the rows where the level is less than -1.3 or greater than 1.5\n",
    "filtered_tide_data_df <- tides_with_date_df %>% filter(level < -1.3 | level > 1.5) %>% select(date, level, tide)\n",
    "\n",
    "# Select the necessary columns from the model_cusum_df dataframe\n",
    "model_data_df <- model_cusum_df %>% select(date, labeled_anomaly, drift_model_cusum)\n",
    "\n",
    "# Merge the model data with the filtered tide data\n",
    "merged_model_tide_df <- left_join(model_data_df, filtered_tide_data_df)\n",
    "\n",
    "# Filter the merged data to keep only the rows where tide is not 'NA' and drift_model_cusum is 'TRUE'\n",
    "filtered_merged_df <- merged_model_tide_df %>% filter(tide != 'NA' & drift_model_cusum == 'TRUE')\n",
    "\n",
    "# Add a 'tide_drift' column to the merged data, which indicates whether the sensor was drifting at the time of each measurement\n",
    "tidal_drift_df <- merged_model_tide_df %>% mutate(tide_drift = ifelse(drift_model_cusum == 'TRUE' & (level < -1.3 | level > 1.5), 'FALSE', drift_model_cusum))\n",
    "\n",
    "# Filter the data to keep only the rows where drift_model_cusum is 'TRUE' and tide_drift is not 'NA'\n",
    "filtered_tidal_drift_df <- tidal_drift_df %>% filter(drift_model_cusum == 'TRUE' & tide_drift != 'NA')\n",
    "\n",
    "# Group the data by date and keep only the unique rows\n",
    "unique_filtered_tidal_drift_df <- filtered_tidal_drift_df %>% group_by(date) %>% unique()\n",
    "\n",
    "# Create a vector of unique dates\n",
    "date_vector <- unique_filtered_tidal_drift_df$date\n",
    "\n",
    "# Check if the dates in model_cusum are in the date_vector\n",
    "date_in_model_cusum <- model_cusum$date %in% date_vector\n",
    "\n",
    "# Convert the result to a dataframe\n",
    "date_in_model_cusum_df <- as.data.frame(date_in_model_cusum)\n",
    "\n",
    "# Merge the model_cusum dataframe with the date_in_model_cusum_df dataframe\n",
    "merged_model_cusum_date_df <- cbind(model_cusum, date_in_model_cusum_df)\n",
    "\n",
    "# Add a 'tides' column to the data, which indicates whether the sensor was drifting and the tide was high or low at the time of each measurement\n",
    "tides_df <- merged_model_cusum_date_df %>% mutate(tides = ifelse(drift_model_cusum == 'TRUE' & date_in_model_cusum == 'TRUE', 'FP', drift_model_cusum))\n",
    "\n",
    "# Filter the data to keep only the rows where tides is 'FP'\n",
    "filtered_tides_df <- tides_df %>% filter(tides == 'FP')\n",
    "\n",
    "# Merging all together\n",
    "\n",
    "# Select the necessary columns from the tides_df, rain, and cali dataframes\n",
    "tides_datetime_df <- tides_df %>% select(datetime, tides)\n",
    "rain_datetime_df <- rain %>% select(datetime, rain)\n",
    "calibration_datetime_df <- cali %>% select(datetime, drift_cal)\n",
    "\n",
    "# Merge the tides and rain data\n",
    "merged_tides_rain_df <- merge(tides_datetime_df, rain_datetime_df, by = 'datetime')\n",
    "\n",
    "# Merge the result with the calibration data\n",
    "merged_all_df <- merge(merged_tides_rain_df, calibration_datetime_df, by = 'datetime')\n",
    "\n",
    "# Filter the data to keep only the rows where tides, rain, or drift_cal is 'FP', then group by datetime and keep only the first row of each group\n",
    "# Finally, add a 'final_d' column which is 'FALSE' for all rows, and select the 'datetime' and 'final_d' columns\n",
    "final_df <- merged_all_df %>% filter(tides == 'FP' | rain == 'FP' | drift_cal == 'FP') %>% group_by(datetime) %>%\n",
    "slice_head() %>% ungroup() %>% mutate(final_d = 'FALSE') %>% select(datetime, final_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27bc2e90-9594-4213-aa35-3eecaf9227b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Wrangling and writing back to Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b955e7-aac8-46fc-821c-fa7e350020aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Select relevant columns\n",
    "\n",
    "# Join model_cusum_df with todo dataframe and select relevant columns\n",
    "model_todo_joined_df <- model_cusum_df %>% select(1:3, 16,18) %>% left_join(todo) %>% ungroup()\n",
    "\n",
    "# If final_d is NA, replace it with the value from drift_model_cusum column\n",
    "model_todo_filled_df <- model_todo_joined_df %>% mutate(final_d = ifelse(is.na(final_d), model_todo_joined_df$drift_model_cusum, model_todo_joined_df$final_d))\n",
    "\n",
    "# Filter rows where drift_model_cusum is 'TRUE'\n",
    "filtered_model_todo_df <- model_todo_filled_df %>% filter(drift_model_cusum == 'TRUE')\n",
    "\n",
    "# Rename with relevant column names\n",
    "\n",
    "# Rename columns starting with \"cond\", \"ph\", \"temp\" and \"sensor\" to \"EC\", \"pH\", \"WaterTemp\" and \"{ss_name}_quality\" respectively\n",
    "# Also, add a new column Location_code with the value of sensor_b\n",
    "renamed_model_todo_df <- model_todo_filled_df %>%\n",
    "rename_if(startsWith(names(.), \"cond\"), ~paste0(\"EC\")) %>%\n",
    "rename_if(startsWith(names(.), \"ph\"), ~paste0(\"pH\")) %>%\n",
    "rename_if(startsWith(names(.), \"temp\"), ~paste0(\"WaterTemp\")) %>%\n",
    "mutate(Location_code = sensor_b)\n",
    "\n",
    "# Get the name of the second column\n",
    "sensor_name <- names(renamed_model_todo_df[2])\n",
    "\n",
    "# Rename the column starting with \"sensor\" to \"{sensor_name}_quality\" and add a new column Location_code with the value of sensor_b\n",
    "quality_renamed_df <- renamed_model_todo_df %>% rename_if(startsWith(names(.), \"sensor\"), ~paste0(glue('{sensor_name}_quality'))) %>% mutate(Location_code = sensor_b)\n",
    "\n",
    "# Gather the \"{sensor_name}_quality\" column into two new columns: measurement_quality and measurement_quality_value\n",
    "quality_gathered_df <- gather(quality_renamed_df, measurement_quality, measurement_quality_value, glue('{sensor_name}_quality'))\n",
    "\n",
    "# Gather the \"{sensor_name}\" column into two new columns: measurement and measurement_value\n",
    "measurement_gathered_df <- gather(quality_gathered_df, measurement, measurement_value, glue('{sensor_name}'))\n",
    "\n",
    "# Convert final_d to logical\n",
    "renamed_model_todo_df$final_d <- as.logical(renamed_model_todo_df$final_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "564e5a65-50f0-43f8-94cd-b9b703fee0e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Test metrics\n",
    "\n",
    "as.integer(as.logical(df6$sensor_qual))\n",
    "as.integer(as.logical(df3$sensor_qual))\n",
    "\n",
    "example2<- ml_test(predicted=df3$final_d, true = df3$labeled_anomaly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4fea20a-7331-46cb-b29b-c226213b7b67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Visualise\n",
    "\n",
    "p <- ggplot(df2, aes(x=datetime, y=df2[,2], col=final_d))+geom_point(size=1) +\n",
    "  scale_color_manual(values = c(\"TRUE\" = \"red\", \"FALSE\" = \"black\"))\n",
    "\n",
    "p <- ggplot(df6, aes(x=datetime, y=measurement_value, col=measurement_quality_value))+geom_point(size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8a43f6b-6453-414e-86ab-66814e4fdc4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Write to blob storage\n",
    "\n",
    "filename = 'test'\n",
    "\n",
    "if(nrow(df6) == 0){\n",
    "  print(\"This data frame is empty\")\n",
    "}else{\n",
    "  d3 <- copy_to(sc, df6, overwrite= TRUE)\n",
    "\n",
    "  spark_write_parquet(\n",
    "  d3,\n",
    "  glue('abfss://lake-raw@etdllake{env}.dfs.core.windows.net/sensor_drift/results2'),\n",
    "  header = TRUE,\n",
    "  delimiter = \",\",\n",
    "  quote = \"\\\"\",\n",
    "  escape = \"\\\\\",\n",
    "  charset = \"UTF-8\",\n",
    "  null_value = NULL,\n",
    "  options = list(),\n",
    "  partition_by = NULL, mode = \"overwrite\"\n",
    ")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "709ab2bb-1a3d-41a0-a30f-51eba6839cd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Write results back to lake to then run aggregates\n",
    "\n",
    "file_path6 <- glue('abfss://lake-raw@etdllake{env}.dfs.core.windows.net/sensor_drift/results2')\n",
    "\n",
    "\n",
    "results_drift <- spark_read_parquet(sc, \n",
    "                            path = file_path6,\n",
    "                            header = TRUE,\n",
    "                            infer_schema = TRUE)\n",
    "\n",
    "results_drift<- collect(results_drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a39c2d7-9f1f-44d0-ab55-4e6b4e71c544",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%r\n",
    "\n",
    "# Register aggregate run\n",
    "\n",
    "# Create a run name by appending '_aggregate' to the name\n",
    "run = glue('{name}_aggregate')\n",
    "\n",
    "# Start a new MLflow run with the specified experiment ID\n",
    "mlflow_start_run(experiment_id= 'ecf79be485134557b8485621a6c18924')\n",
    "\n",
    "# Set the run name tag for the current MLflow run\n",
    "mlflow_set_tag(\"mlflow.runName\",run)\n",
    "\n",
    "# Log the first value of the 'F2' column from the 'example2' dataframe as a metric in the current MLflow run\n",
    "mlflow_log_metric(\"f2 events\", example2$F2[1])\n",
    "\n",
    "# Log the second value of the 'F2' column from the 'example2' dataframe as a metric in the current MLflow run\n",
    "mlflow_log_metric(\"f2 observations)\", example2$F2[2])\n",
    "\n",
    "# End the current MLflow run\n",
    "mlflow_end_run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 378754985970450,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sensor_drift_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
